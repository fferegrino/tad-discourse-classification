{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2gv2i9fNDvrg"
   },
   "source": [
    "## Part A: Subreddit Prediction ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qYOgC4D1k9eX"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "except:\n",
    "    import files\n",
    "    \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119,
     "output_extras": [
      {
       "item_id": 4
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3198,
     "status": "ok",
     "timestamp": 1520765763773,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "ZG9BpbQt3-ko",
    "outputId": "cf89af53-0546-4c68-990c-dd41dcc852cb"
   },
   "outputs": [],
   "source": [
    "subreddit_train = \"coursework_subreddit_train.json\"\n",
    "subreddit_test = \"coursework_subreddit_test.json\"\n",
    "\n",
    "!curl -o  $subreddit_train https://storage.googleapis.com/tad2018/coursework_subreddit_train3.json\n",
    "!curl -o  $subreddit_test https://storage.googleapis.com/tad2018/coursework_subreddit_test3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1520765764705,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "iCEG8t6PC2f7",
    "outputId": "bc242691-01a1-4281-ad9b-c10ba29c5d4b"
   },
   "outputs": [],
   "source": [
    "train_threads = pd.read_json(path_or_buf=subreddit_train, lines=True)\n",
    "train_threads.subreddit = pd.Categorical(train_threads.subreddit)\n",
    "#print(train_threads.head())\n",
    "print(train_threads.size)\n",
    "print(train_threads.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "89UU3g27C8SZ"
   },
   "outputs": [],
   "source": [
    "test_threads = pd.read_json(path_or_buf=subreddit_test, lines=True)\n",
    "test_threads.subreddit = pd.Categorical(test_threads.subreddit)\n",
    "#print(test_threads.head())\n",
    "print(test_threads.size)\n",
    "print(test_threads.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3Nl9qzazDQ_6"
   },
   "outputs": [],
   "source": [
    "subreddit_counts = train_threads['subreddit'].value_counts()\n",
    "#print(subreddit_counts.describe())\n",
    "top_subbreddits = subreddit_counts.nlargest(20)\n",
    "top_subbreddits_list = top_subbreddits.index.tolist()\n",
    "#print(top_subbreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "02OXNfo9H8Kc"
   },
   "outputs": [],
   "source": [
    "train_labels = train_threads['subreddit']\n",
    "test_labels = test_threads['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSEU-93lqxvS"
   },
   "source": [
    "### General purpose code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "iMX1SI_JLTXN"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def eval_summary(predictions, labels, avg='macro'):\n",
    "    precision = precision_score(predictions, labels, average=avg)\n",
    "    recall = recall_score(predictions, labels, average=avg)\n",
    "    f1 = fbeta_score(predictions, labels, 1, average=avg)\n",
    "    accuracy = accuracy_score(predictions, labels)\n",
    "    \n",
    "    summary = [accuracy, precision, recall, f1]\n",
    "    report = classification_report(predictions, labels)\n",
    "    matrix= confusion_matrix(predictions, labels)\n",
    "    \n",
    "    return (summary, report, matrix)\n",
    "\n",
    "def dataframe_from_report(report, index_name=None):\n",
    "    lines = [l.strip() for l in report.split(\"\\n\") if l.strip() != '']\n",
    "    column_names = [l for l in lines[0].split(\" \") if l != '']\n",
    "    pre_df = {}\n",
    "    for line in lines[1:-1]:\n",
    "        spl = line.split(\" \")\n",
    "        values = [float(l) for l in spl[1:] if l != '']\n",
    "        pre_df[spl[0]] = values\n",
    "    data = pd.DataFrame.from_dict(pre_df, orient='index')\n",
    "    data.columns=column_names\n",
    "    data.index.name=index_name\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1520765774808,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "21DGM8lbLTXD",
    "outputId": "c82426b2-c560-4f6a-8b78-6487204615fc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(string):\n",
    "  normalized_tokens = list()\n",
    "  tokens = regexp_tokenizer.tokenize(string)\n",
    "  for token in tokens:\n",
    "    if token.lower() not in stop_words and len(token) > 1 and len(token) < 24:\n",
    "      normalized = token.lower()\n",
    "      #normalized = stemmer.stem(normalized)\n",
    "      normalized_tokens.append(normalized)\n",
    "  return normalized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWcZm0xeLTXC"
   },
   "source": [
    "### Q1  \n",
    "Use the text of the thread (title) and body of all comments in a thread to learn a classification model, based on the Scikit Learn package. Your labels are the subreddit that the thread came from. You should conduct experiments using two vectorizers:  \n",
    "\n",
    " - CountVectorizer\n",
    " - TfidfVectorizer\n",
    " \n",
    "and two classifiers:  \n",
    "\n",
    " - LogisticRegression\n",
    " - SVCClassifier (i.e. SVM)\n",
    " \n",
    "Evaluate the classifiers (and two `DummyClassifiers`, with `strategy=\"most_frequent\"` and `strategy=\"stratified\"`) using Accuracy, Precision, Recall & F1 measures (macro averages). Report your results for these values in a single table. Also, produce a confusion_matrix for the best classifier. Create a bar chart graph with the F1 score on the Y-axis and the subreddit name on the X-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1520722936241,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "LJlVNc6aLTXL",
    "outputId": "7661d53f-9a59-4d0e-9524-8f3f48385d08"
   },
   "outputs": [],
   "source": [
    "def get_text_from_thread(thread):\n",
    "    texts_for_thread = [thread['title']]\n",
    "    for post in thread['posts']:\n",
    "        if \"body\" in post:\n",
    "            t = post['body'].strip()\n",
    "            if t != '':\n",
    "                texts_for_thread.append(t)\n",
    "    return \" \".join(texts_for_thread)\n",
    "  \n",
    "train_threads['full_thread'] = train_threads.apply(get_text_from_thread, axis=1)\n",
    "test_threads['full_thread'] = test_threads.apply(get_text_from_thread, axis=1)\n",
    "\n",
    "full = pd.concat([train_threads['full_thread'], test_threads['full_thread']])\n",
    "print(len(full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "B81M-tpSLTXI"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize)\n",
    "count_vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "\n",
    "train_tfidf_matrix = tfidf_vectorizer.fit_transform(train_threads.full_thread.values)\n",
    "train_cv_matrix = count_vectorizer.fit_transform(train_threads.full_thread.values)\n",
    "\n",
    "test_tfidf_matrix = tfidf_vectorizer.transform(test_threads.full_thread.values)\n",
    "test_cv_matrix = count_vectorizer.transform(test_threads.full_thread.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 884,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 988,
     "status": "ok",
     "timestamp": 1520723824161,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "vIRfqDTxJrYT",
    "outputId": "8b210fcb-a1f7-481a-cdbb-22b5c3239ab0"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "all_tokens = []\n",
    "for l in train_threads.full_thread.values:\n",
    "  all_tokens.extend(tokenize(l))\n",
    "print(len(all_tokens), len(train_threads.full_thread.values))\n",
    "word_dist = nltk.FreqDist(all_tokens)\n",
    "\n",
    "word_dist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10787,
     "status": "ok",
     "timestamp": 1519909382876,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "8VzvLsEVLTXT",
    "outputId": "c58cc762-8671-4cb5-ad44-455e7778143b"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "\n",
    "most_frequent_cv = DummyClassifier(strategy=\"most_frequent\")\n",
    "most_frequent_cv.fit(train_cv_matrix, train_labels)\n",
    "\n",
    "stratified_cv = DummyClassifier(strategy=\"stratified\")\n",
    "stratified_cv.fit(train_cv_matrix, train_labels)\n",
    "\n",
    "lr_cv = LogisticRegression()\n",
    "lr_cv.fit(train_cv_matrix, train_labels)\n",
    "\n",
    "svc_cv = SVC()\n",
    "svc_cv.fit(train_cv_matrix, train_labels)\n",
    "\n",
    "\n",
    "most_frequent_tfidf = DummyClassifier(strategy=\"most_frequent\")\n",
    "most_frequent_tfidf.fit(train_tfidf_matrix, train_labels)\n",
    "\n",
    "stratified_tfidf = DummyClassifier(strategy=\"stratified\")\n",
    "stratified_tfidf.fit(train_tfidf_matrix, train_labels)\n",
    "\n",
    "svc_tfidf = SVC()\n",
    "svc_tfidf.fit(train_tfidf_matrix, train_labels)\n",
    "\n",
    "lr_tfidf = LogisticRegression()\n",
    "lr_tfidf.fit(train_tfidf_matrix, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 139,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1740,
     "status": "ok",
     "timestamp": 1519909389376,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "RqSUzQUaLTXV",
    "outputId": "c01b32bf-4744-44d1-fc96-d16892dc7283"
   },
   "outputs": [],
   "source": [
    "summaries_list = []\n",
    "\n",
    "a = ['Dummy Stratified', 'Count Vectorizer']\n",
    "prediction = stratified_cv.predict(test_cv_matrix)\n",
    "summary,_,_ = eval_summary(prediction, test_labels, avg='macro')\n",
    "a.extend(summary)\n",
    "summaries_list.append(a)\n",
    "\n",
    "a = ['Dummy Stratified', 'Tf-idf Vectorizer']\n",
    "prediction = stratified_tfidf.predict(test_tfidf_matrix)\n",
    "summary,_,_ = eval_summary(prediction, test_labels, avg='macro')\n",
    "a.extend(summary)\n",
    "summaries_list.append(a)\n",
    "\n",
    "a = ['Dummy Most_Frequent', 'Count Vectorizer']\n",
    "prediction = most_frequent_cv.predict(test_cv_matrix)\n",
    "summary,_,_ = eval_summary(prediction, test_labels, avg='macro')\n",
    "a.extend(summary)\n",
    "summaries_list.append(a)\n",
    "\n",
    "a = ['Dummy Most_Frequent', 'Tf-idf Vectorizer']\n",
    "prediction = most_frequent_tfidf.predict(test_tfidf_matrix)\n",
    "summary,_,_ = eval_summary(prediction, test_labels, avg='macro')\n",
    "a.extend(summary)\n",
    "summaries_list.append(a)\n",
    "\n",
    "a = ['LogisticRegression', 'Count Vectorizer']\n",
    "prediction = lr_cv.predict(test_cv_matrix)\n",
    "summary,_,_ = eval_summary(prediction, test_labels, avg='macro')\n",
    "a.extend(summary)\n",
    "summaries_list.append(a)\n",
    "\n",
    "a = ['LogisticRegression', 'Tf-idf Vectorizer']\n",
    "prediction = lr_tfidf.predict(test_tfidf_matrix)\n",
    "summary,_,_ = eval_summary(prediction, test_labels, avg='macro')\n",
    "a.extend(summary)\n",
    "summaries_list.append(a)\n",
    "\n",
    "a = ['SVM Classifier', 'Count Vectorizer']\n",
    "prediction = svc_cv.predict(test_cv_matrix)\n",
    "summary,_,_ = eval_summary(prediction, test_labels, avg='macro')\n",
    "a.extend(summary)\n",
    "summaries_list.append(a)\n",
    "\n",
    "a = ['SVM Classifier', 'Tf-idf Vectorizer']\n",
    "prediction = svc_tfidf.predict(test_tfidf_matrix)\n",
    "summary,_,_ = eval_summary(prediction, test_labels, avg='macro')\n",
    "a.extend(summary)\n",
    "summaries_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 297,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2259,
     "status": "ok",
     "timestamp": 1519909404690,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "XuuF_XRkLTXY",
    "outputId": "3b75afd8-e916-44a6-bf8d-513c84fe7862"
   },
   "outputs": [],
   "source": [
    "summaries = pd.DataFrame(summaries_list)\n",
    "summaries.columns =   ['Model Name', 'Vectorizer', 'Accuracy', 'Precision', 'Recall', 'F-1']\n",
    "summaries = summaries.sort_values(by=[\"F-1\"], ascending=False)\n",
    "summaries.to_csv('summarized_performance.csv')\n",
    "files.download('summarized_performance.csv')\n",
    "summaries.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wIDXS2mRLTXb"
   },
   "outputs": [],
   "source": [
    "best = lr_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1561,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      },
      {
       "item_id": 3
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5401,
     "status": "ok",
     "timestamp": 1519912573469,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "5OR1uRDBLTXj",
    "outputId": "92626189-be3b-4884-8dcf-dc5c1833668f"
   },
   "outputs": [],
   "source": [
    "prediction = best.predict(test_cv_matrix)\n",
    "summary,report,matrix = eval_summary(prediction, test_labels, avg='macro')\n",
    "c_report = dataframe_from_report(report, 'subreddit')\n",
    "\n",
    "c_matrix = pd.DataFrame(matrix, columns=c_report.index, index=c_report.index)\n",
    "c_matrix.to_csv('confusion_matrix.csv')\n",
    "#files.download('confusion_matrix.csv')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(c_report)\n",
    "print(matrix)\n",
    "c_report.to_csv('best_report.csv')\n",
    "#files.download('best_report.csv')\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(range(len(c_report)),c_report['f1-score'])\n",
    "plt.xticks(range(len(c_report)), c_report.index, rotation='vertical', fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"subreddit\", fontsize=20)\n",
    "plt.ylabel(\"F1-score\", fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('barchart.png')\n",
    "files.download('barchart.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zLGFEjkid4mj"
   },
   "source": [
    "### Q2  \n",
    "\n",
    "Parameter tuning. We have previously discussed some parameters such as sublinear_tf, n-gram sizes, and vocabulary sizes in the TfidfVectorizer. Tune the vectorizer & classifier parameters on the training data, namely (Hint: You can use SKlearn's `GridSearchCV`):\n",
    "\n",
    " - `TfidfVectorizer`: `sublinear_tf=True` or `False`; n-gram 1 to 3; vocabulary size $5000, 10000, 20000, ALL$\n",
    " - `LogisticRegression`: `C` (a regularisation parameter, values $C = 0.1, 1, 10, 100$)\n",
    " - `SVCClassifier`: `C` (a penalty parameter, values $C = 0.1, 1, 10, 100$)  \n",
    " \n",
    "Report the best parameters found and the results of the model with those parameters on the test data.\n",
    "\n",
    "### WARNING: this may take a lot to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 9370,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 29
      },
      {
       "item_id": 58
      },
      {
       "item_id": 86
      },
      {
       "item_id": 114
      },
      {
       "item_id": 142
      },
      {
       "item_id": 170
      },
      {
       "item_id": 199
      },
      {
       "item_id": 227
      },
      {
       "item_id": 255
      },
      {
       "item_id": 269
      },
      {
       "item_id": 270
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 685275,
     "status": "ok",
     "timestamp": 1519840023963,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "ugMYp9oTfGST",
    "outputId": "d684abf2-8ffe-47db-9519-82dbe7cae8b2"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # I know there will be some warnings...\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=tokenize)),\n",
    "    ('classify', SVC())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vectorizer__sublinear_tf': [True, False],\n",
    "    'vectorizer__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'vectorizer__max_features': [5000, 10000, 20000],\n",
    "    \n",
    "    # Different classifiers:\n",
    "    'classify': [LogisticRegression()],\n",
    "    'classify__C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=4, n_jobs=-1, param_grid=params, verbose=1, scoring='f1_macro')\n",
    "\n",
    "grid.fit(train_threads.full_thread.values, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 258,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 496,
     "status": "ok",
     "timestamp": 1519840329186,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "E6PScEimeFcI",
    "outputId": "8a8f03db-49f8-4896-c442-db670b00409d"
   },
   "outputs": [],
   "source": [
    "print(grid.best_score_)\n",
    "print()\n",
    "print(grid.best_params_)\n",
    "print()\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 799,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1519840354337,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "LHxR0E8IxdGd",
    "outputId": "06295927-01aa-496d-c0ed-ec5599ce1564"
   },
   "outputs": [],
   "source": [
    "best_estimator = grid.best_estimator_\n",
    "\n",
    "\n",
    "prediction = best_estimator.predict(test_threads.full_thread.values)\n",
    "report = dataframe_from_report(classification_report(prediction, test_labels), 'subreddit')\n",
    "matrix= confusion_matrix(prediction, test_labels)\n",
    "\n",
    "summary, report, matrix = eval_summary( prediction, test_labels)\n",
    "\n",
    "print(summary)\n",
    "print(report)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-G938e8D5qn"
   },
   "source": [
    "## Part B: Discourse prediction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Q45_eXsmYb9"
   },
   "source": [
    "### Load data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119,
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4355,
     "status": "ok",
     "timestamp": 1520764225133,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "9KM6aJtSETPY",
    "outputId": "152ae948-1f00-4db8-b653-1bc554a87046"
   },
   "outputs": [],
   "source": [
    "discourse_train = \"coursework_discourse_train.json\"\n",
    "discourse_test = \"coursework_discourse_test.json\"\n",
    "\n",
    "!curl -o  $discourse_train https://storage.googleapis.com/tad2018/coursework_discourse_train.json\n",
    "!curl -o  $discourse_test https://storage.googleapis.com/tad2018/coursework_discourse_test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "pwOaf_6aD-Vh"
   },
   "outputs": [],
   "source": [
    "# The reddit thread structure is nested with posts in a new content.\n",
    "# This block reads the file as json and creates a new data frame.\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_posts(file):\n",
    "  # A temporary variable to store the list of post content.\n",
    "  posts_tmp = list()\n",
    "\n",
    "  with open(file) as jsonfile:\n",
    "    for i, line in enumerate(jsonfile):\n",
    "     # if (i > 2): break\n",
    "      thread = json.loads(line)\n",
    "      for post in thread['posts']:\n",
    "        posts_tmp.append((thread['subreddit'], thread['title'], thread['url'],\n",
    "                        post['id'], post.get('author', \"\"), post.get('body', \"\"), post.get(\"majority_link\", \"\"), \n",
    "                        post.get('post_depth', 0), post.get('author', \"\"), post.get('majority_type', \"\"), post.get('in_reply_to', \"\") ))\n",
    "\n",
    "# Create the posts data frame.  \n",
    "  labels = ['subreddit', 'title', 'url', 'id', 'author', 'body', 'majority_link', \n",
    "          'post_depth', 'author', 'discourse_type', 'in_reply_to']\n",
    "  return pd.DataFrame(posts_tmp, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1618,
     "status": "ok",
     "timestamp": 1520764227510,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "PDzHTDcmEQ11",
    "outputId": "8d55c671-367e-4b9b-8bda-9f7d664c3d8f"
   },
   "outputs": [],
   "source": [
    "train_posts = load_posts(discourse_train)\n",
    "# Filter out empty labels\n",
    "train_posts = train_posts[train_posts['discourse_type'] != \"\"]\n",
    "\n",
    "print(train_posts[[\"body\",\"discourse_type\"]].head())\n",
    "print(\"Num posts: \", len(train_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqGLzyTOGadY"
   },
   "source": [
    "The label for the post we will be predicting is in the discourse_type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1520764228368,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "7vvo7hfCEmvj",
    "outputId": "c9d7a790-8de9-41cd-8dfd-2eafe5384a0a"
   },
   "outputs": [],
   "source": [
    "test_posts = load_posts(discourse_test)\n",
    "# Filter out empty labels\n",
    "test_posts = test_posts[test_posts['discourse_type'] != \"\"]\n",
    "\n",
    "print(\"Num posts: \", len(test_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Jat55HhNHGBp"
   },
   "outputs": [],
   "source": [
    "train_posts['discourse_type'] = pd.Categorical(train_posts['discourse_type'])\n",
    "test_posts['discourse_type'] = pd.Categorical(test_posts['discourse_type'])\n",
    "train_labels = train_posts['discourse_type']\n",
    "test_labels = test_posts['discourse_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFl6sM58HNFp"
   },
   "source": [
    "Examine the distribution over labels on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 394,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1520764231899,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "n3NbLPBhFOkp",
    "outputId": "fa9efd9c-af4c-4fc3-c2a1-2e4b16742fcd"
   },
   "outputs": [],
   "source": [
    "discourse_counts = train_labels.value_counts()\n",
    "print(discourse_counts.describe())\n",
    "\n",
    "top_discourse = discourse_counts.nlargest(200)\n",
    "print(top_discourse)\n",
    "top_discourse = top_discourse.index.tolist()\n",
    "print(top_discourse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBOOABMu1TMF"
   },
   "source": [
    "### Q3\n",
    "\n",
    "Build and evaluate a text classification model for comment discourse prediction. You should use the best vector representation (CountVectorizer/TfIdfVectorizer) identified in Q1 above, as input to a LogisticRegression classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "M7Ow3DCh5KZb"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "\n",
    "train_posts_ = train_posts.body\n",
    "test_posts_ = test_posts.body\n",
    "\n",
    "train_matrix = vectorizer.fit_transform(train_posts_.values)\n",
    "test_matrix = vectorizer.transform(test_posts_.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 224,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 440,
     "status": "ok",
     "timestamp": 1520765805498,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "C9mH44INpjib",
    "outputId": "d7e9915d-ff94-479c-db89-d30c23692859"
   },
   "outputs": [],
   "source": [
    "print(train_posts_[0])\n",
    "print(train_posts_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "pbVa1fkZGHVQ"
   },
   "outputs": [],
   "source": [
    "# Define the features\n",
    "X_train = train_matrix\n",
    "X_test = test_matrix\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, train_posts['discourse_type'])\n",
    "predictions = lr.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 510,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4607,
     "status": "ok",
     "timestamp": 1520765953189,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "0woiv41N0la4",
    "outputId": "b4feb47a-3afb-41aa-ee44-a68be8ac78fd"
   },
   "outputs": [],
   "source": [
    "summary, report, matrix = eval_summary(predictions, test_posts['discourse_type'])\n",
    "\n",
    "c_summary = pd.DataFrame([summary], columns=['Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "\n",
    "c_summary.to_csv('c_summary_q3.csv')\n",
    "files.download('c_summary_q3.csv')\n",
    "\n",
    "c_report = dataframe_from_report(report, 'subreddit')\n",
    "c_matrix = pd.DataFrame(matrix, columns=c_report.index, index=c_report.index)\n",
    "\n",
    "c_matrix.to_csv('confusion_matrix_q3.csv')\n",
    "files.download('confusion_matrix_q3.csv')\n",
    "\n",
    "c_report.to_csv('report_q3.csv')\n",
    "files.download('report_q3.csv')\n",
    "\n",
    "print(c_summary)\n",
    "print()\n",
    "print(report)\n",
    "print()\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyg8WkSW1iZL"
   },
   "source": [
    "### Q4\n",
    "\n",
    "Discourse classification may be more subtle than subreddit classification. There are many different types of features that could be used to improve the effectiveness of discourse classification:\n",
    "\n",
    " - Content + Punctuation\n",
    "    - Words unigrams, bigrams, and trigrams\n",
    "    - Using the title of the comment (when it’s the first comment)\n",
    "    - Character n-grams (n=1,2,3,4)\n",
    "    - A tokenizer that includes punctuation as tokens instead of removing it (including \"?\" and \"!\")\n",
    " - Structure\n",
    "    - The depth of the comment, raw or normalized by the length of the thread\n",
    "    - The number of sentences, number of words, and number of characters of both the body and the title of the comment\n",
    " - Author\n",
    "    - A binary feature for whether the current author is also the author of the initial post\n",
    "    - A binary feature for whether the current commenter is the same as the parent commenter\n",
    " - Thread features\n",
    "    - The total number of comments in the discussion\n",
    "    - Number of unique branches in the discussion tree\n",
    "    - Whether the discussion originated as a self-post or a link-post\n",
    "    - Average length of all the branches or threads of discussion in the discussion tree\n",
    " - Community\n",
    "    - The subreddit the post came from\n",
    " - Word2Vec / NLP\n",
    "    - Average/ Max of pre-trained Glove embeddings for the post\n",
    "    - Average/ Max of gensim word2vec reddit embeddings for the post\n",
    "    - Whether the post contains particular part-of-speech n-grams (n=2,3,4)\n",
    "\n",
    "Implement one feature from each of the 6 different feature types above. Combine all features together into a single model, then evaluate; Then ablate the features by learning a model while \"leaving one-out\", and report the performances. The required evaluation measures and layout of results to include in your report are described below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "oMieWn2G8btp"
   },
   "outputs": [],
   "source": [
    "def load_posts_rich(file):\n",
    "  # A temporary variable to store the list of post content.\n",
    "  posts_tmp = list()\n",
    "\n",
    "  with open(file) as jsonfile:\n",
    "    for i, line in enumerate(jsonfile):\n",
    "      thread = json.loads(line)\n",
    "      thread_author = None\n",
    "      for post in thread['posts']:\n",
    "        # NOTE: This could be changed to use additional features from the post or thread.\n",
    "        # DO NOT change the labels for the test set.\n",
    "        discourse_type = post.get('majority_type', '')\n",
    "        \n",
    "        post_depth = 0\n",
    "        if 'is_first_post' in post and post['is_first_post']:\n",
    "          thread_author = post.get('author', None)\n",
    "        else:\n",
    "          post_depth = post['post_depth']\n",
    "          \n",
    "        features = [\n",
    "            thread[\"is_self_post\"],          # is_self_post\n",
    "            len(thread['posts']),            # thread_length\n",
    "            post_depth,                      # post_depth\n",
    "            post.get('author', None),        # post_author\n",
    "            thread_author,                   # thread_author\n",
    "            thread['subreddit'],             # subreddit\n",
    "            thread['title'],                 # thread_title\n",
    "            post.get('body',''),             # body\n",
    "            discourse_type,                  # discourse_type\n",
    "        ]\n",
    "        posts_tmp.append(features)\n",
    "\n",
    "  # Create the posts data frame.  \n",
    "  labels = [\n",
    "      'is_self_post',\n",
    "      'thread_length',\n",
    "      'post_depth',\n",
    "      'post_author',\n",
    "      'thread_author',\n",
    "      'subreddit',\n",
    "      'thread_title',\n",
    "      'body',\n",
    "      'discourse_type'\n",
    "  ]\n",
    "  return pd.DataFrame(posts_tmp, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1980,
     "status": "ok",
     "timestamp": 1520511629257,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "ypgy6nK0Qs3L",
    "outputId": "5c19227f-132d-4983-c9f5-d3c6b68341a7"
   },
   "outputs": [],
   "source": [
    "train_posts = load_posts_rich(discourse_train)\n",
    "train_posts = train_posts[train_posts['discourse_type'] != \"\"]\n",
    "\n",
    "test_posts = load_posts_rich(discourse_test)\n",
    "test_posts = test_posts[test_posts['discourse_type'] != \"\"]\n",
    "\n",
    "#train_posts = train_posts.sample(int(len(train_posts) * 0.3))\n",
    "#test_posts = test_posts.sample(int(len(test_posts) * 0.3))\n",
    "\n",
    "train_labels = train_posts['discourse_type']\n",
    "test_labels = test_posts['discourse_type']\n",
    "\n",
    "#print(train_posts.head())\n",
    "print()\n",
    "print(\"Train size\", len(train_posts))\n",
    "print(\"Test size\", len(test_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4967,
     "status": "ok",
     "timestamp": 1520511634348,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "WemcxpsI2VrL",
    "outputId": "747ce357-88aa-4c8a-96f6-ac435e54134e"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/erikavaris/tokenizer.git\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tokenizer import tokenizer as r_tokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1520511635077,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "uT5CTAWQRaRE",
    "outputId": "8945847f-2a75-4a77-fb8e-80541f4b5089"
   },
   "outputs": [],
   "source": [
    "stopw = set(stopwords.words('english'))\n",
    "stopw = stopw - set(['who', 'where','when','why', 'what', 'which', 'how'])\n",
    "stopw = stopw - set(['no', 'not','doesn\\'t'])\n",
    "stopw = stopw - set(['i','are','is', 'it'])\n",
    "\n",
    "R = r_tokenizer.RedditTokenizer()\n",
    "\n",
    "def tknze(string):\n",
    "  normalized_tokens = list()\n",
    "  tokens = R.tokenize(string)\n",
    "  for t in tokens:\n",
    "    normalized = t.lower()\n",
    "    if normalized in stopw:\n",
    "      continue\n",
    "#    if len(normalized) < 30:\n",
    "    normalized_tokens.append(normalized)\n",
    "  return normalized_tokens\n",
    "\n",
    "tknze(\"How is it going?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51,
     "output_extras": [
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26018,
     "status": "ok",
     "timestamp": 1520511661481,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "Ax9qtsXQM99C",
    "outputId": "e86af607-8e5e-463e-a0c5-5a2df8d419b8"
   },
   "outputs": [],
   "source": [
    "# Train Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "nltk.download('treebank')\n",
    "corpus = nltk.corpus.treebank\n",
    "treebank_tagged_sentences = corpus.tagged_sents()\n",
    "tagged_sentences = [sentence for sentence in treebank_tagged_sentences]\n",
    "tagger = nltk.tag.perceptron.PerceptronTagger(load=False)\n",
    "tagger.train(tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QgSEv9sBM99K"
   },
   "outputs": [],
   "source": [
    "def get_pos(string):\n",
    "  tags = [tag for word, tag in tagger.tag([a for a in string.split(' ') if a != ''])]\n",
    "  transformed = \" \".join(tags)\n",
    "  return transformed\n",
    "\n",
    "train_posts['pos'] = train_posts['body'].apply(get_pos)\n",
    "test_posts['pos'] = test_posts['body'].apply(get_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Z6Lqym0rR4ao"
   },
   "outputs": [],
   "source": [
    "# Vectorizer & text-based features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "post_vectorizer = TfidfVectorizer(tokenizer=tknze, max_features= 10000, ngram_range= (1, 2), sublinear_tf= True)\n",
    "\n",
    "def get_text(df):\n",
    "  return df['body'].values \n",
    "\n",
    "select_text = FunctionTransformer(get_text, validate=False)\n",
    "\n",
    "content_punctuation_pipeline = Pipeline([\n",
    "      ('select_text', select_text), \n",
    "      ('vectorizer', post_vectorizer)\n",
    "])\n",
    "\n",
    "normal_vectorizer =  TfidfVectorizer(tokenizer=tokenize, sublinear_tf= True)\n",
    "normal_pipeline = Pipeline([\n",
    "      ('select_text', select_text), \n",
    "      ('vectorizer_normal', normal_vectorizer)\n",
    "])\n",
    "\n",
    "# Structure: The depth of the comment, raw or normalized by the length of the thread\n",
    "def get_post_structure(df):\n",
    "  values = []\n",
    "  for i, r in df.iterrows():\n",
    "    sentences = sent_tokenize(r['body'])\n",
    "    words = word_tokenize(r['body'])\n",
    "    length = len(r['body'])\n",
    "    values.append([len(sentences), len(words), length])\n",
    "  return values\n",
    "post_structure = FunctionTransformer(get_post_structure, validate=False)\n",
    "\n",
    "# Author: A binary feature for whether the current author is also the author of the initial post\n",
    "import numpy as np\n",
    "def is_same_author(df):\n",
    "  lst = []\n",
    "  for i, r in df.iterrows():\n",
    "    lst.append(1 if r['post_author'] == r['thread_author'] else 0)\n",
    "  return np.array(lst).reshape(len(lst), 1)\n",
    "same_author = FunctionTransformer(is_same_author, validate=False)\n",
    "\n",
    "# Thread features: The total number of comments in the discussion\n",
    "def get_thread_length(df):\n",
    "  return df['thread_length'].values.reshape(len(df),1)\n",
    "thread_length = FunctionTransformer(get_thread_length, validate=False)\n",
    "\n",
    "# Community: The subreddit the post came from\n",
    "def get_subreddit(df):    \n",
    "  return df['subreddit'].values \n",
    "subreddit_hash_function = FunctionTransformer(get_subreddit, validate=False)\n",
    "\n",
    "subreddit_hash = Pipeline([\n",
    "      ('get_subrr', subreddit_hash_function), \n",
    "      ('vectorizer', CountVectorizer())\n",
    "])\n",
    "\n",
    "# Word2Vec / NLP\n",
    "tokenizer_dashes = RegexpTokenizer(r'\\w+')\n",
    "def simple_tokenizer(string):\n",
    "  return [token.lower() for token in tokenizer_dashes.tokenize(string)]\n",
    "\n",
    "w2v_vectorizer = TfidfVectorizer(tokenizer=simple_tokenizer, sublinear_tf= True, ngram_range= (1, 2), max_features=5000)\n",
    "\n",
    "def get_pos(df):\n",
    "  return df['pos'].values\n",
    "\n",
    "select_pos = FunctionTransformer(get_pos, validate=False)\n",
    "\n",
    "word_2_vec = Pipeline([\n",
    "      ('select_pos', select_pos), \n",
    "      ('w2v_vectorizer', w2v_vectorizer)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 589,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 271694,
     "status": "error",
     "timestamp": 1520512163727,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "j8A91_aVVvX-",
    "outputId": "2c0ab061-a072-4187-d9f5-bf568053f9c2"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "experiments = {\n",
    "    'all': Pipeline([\n",
    "            ('union',FeatureUnion([\n",
    "              ('Content + Punctuation', content_punctuation_pipeline),\n",
    "              ('Structure', post_structure),\n",
    "              ('Author', same_author),\n",
    "              ('Thread features', thread_length),\n",
    "              ('Community', subreddit_hash),\n",
    "              ('Word2vec', word_2_vec)\n",
    "            ])),\n",
    "            ('classify', LogisticRegression())]),\n",
    "    'no_w2v': Pipeline([\n",
    "            ('union',FeatureUnion([\n",
    "              ('Content + Punctuation', content_punctuation_pipeline),\n",
    "              ('Structure', post_structure),\n",
    "              ('Author', same_author),\n",
    "              ('Thread features', thread_length),\n",
    "              ('Community', subreddit_hash),\n",
    "              #('Word2vec', word_2_vec)\n",
    "            ])),\n",
    "            ('classify', LogisticRegression())]),\n",
    "    'no_community': Pipeline([\n",
    "            ('union',FeatureUnion([\n",
    "              ('Content + Punctuation', content_punctuation_pipeline),\n",
    "              ('Structure', post_structure),\n",
    "              ('Author', same_author),\n",
    "              ('Thread features', thread_length),\n",
    "              #('Community', subreddit_hash),\n",
    "              ('Word2vec', word_2_vec)\n",
    "            ])),\n",
    "            ('classify', LogisticRegression())]),\n",
    "    'no_thread': Pipeline([\n",
    "            ('union',FeatureUnion([\n",
    "              ('Content + Punctuation', content_punctuation_pipeline),\n",
    "              ('Structure', post_structure),\n",
    "              ('Author', same_author),\n",
    "              #('Thread features', thread_length),\n",
    "              ('Community', subreddit_hash),\n",
    "              ('Word2vec', word_2_vec)\n",
    "            ])),\n",
    "            ('classify', LogisticRegression())]),\n",
    "    'no_author': Pipeline([\n",
    "            ('union',FeatureUnion([\n",
    "              ('Content + Punctuation', content_punctuation_pipeline),\n",
    "              ('Structure', post_structure),\n",
    "              #('Author', same_author),\n",
    "              ('Thread features', thread_length),\n",
    "              ('Community', subreddit_hash),\n",
    "              ('Word2vec', word_2_vec)\n",
    "            ])),\n",
    "            ('classify', LogisticRegression())]),\n",
    "    'no_structure': Pipeline([\n",
    "            ('union',FeatureUnion([\n",
    "              ('Content + Punctuation', content_punctuation_pipeline),\n",
    "              #('Structure', post_structure),\n",
    "              ('Author', same_author),\n",
    "              ('Thread features', thread_length),\n",
    "              ('Community', subreddit_hash),\n",
    "              ('Word2vec', word_2_vec)\n",
    "            ])),\n",
    "            ('classify', LogisticRegression())]),\n",
    "    'no_structure': Pipeline([\n",
    "            ('union',FeatureUnion([\n",
    "              ('Content + Punctuation', normal_pipeline),\n",
    "              ('Structure', post_structure),\n",
    "              ('Author', same_author),\n",
    "              ('Thread features', thread_length),\n",
    "              ('Community', subreddit_hash),\n",
    "              ('Word2vec', word_2_vec)\n",
    "            ])),\n",
    "            ('classify', LogisticRegression())]),\n",
    "}\n",
    "\n",
    "for experiment in experiments:\n",
    "  print(experiment)\n",
    "  experiments[experiment].fit(train_posts, train_labels)\n",
    "  predictions = experiments[experiment].predict(test_posts)  \n",
    "  \n",
    "  \n",
    "  summary, report, matrix = eval_summary(predictions, test_posts['discourse_type'])\n",
    "\n",
    "  c_report = dataframe_from_report(report, 'subreddit')\n",
    "\n",
    "  c_report.to_csv('report_q4_%s.csv' % experiment)\n",
    "  files.download ('report_q4_%s.csv' % experiment)\n",
    "\n",
    "  c_matrix = pd.DataFrame(matrix, columns=c_report.index, index=c_report.index)\n",
    "  print('confusion_matrix_q4_%s.csv' % datos)\n",
    "  c_matrix.to_csv('confusion_matrix_q4_%s.csv' % experiment)\n",
    "  files.download ('confusion_matrix_q4_%s.csv' % experiment)\n",
    "\n",
    "  c_summary = pd.DataFrame.from_dict({\n",
    "      'accuracy' : [summary[0]],\n",
    "      'precission': [summary[1]],\n",
    "      'recall': [summary[2]],\n",
    "      'f1': [summary[3]]\n",
    "  })\n",
    "\n",
    "  c_summary.to_csv('summary_q4_%s.csv' % experiment)\n",
    "  files.download ('summary_q4_%s.csv' % experiment)\n",
    "\n",
    "  print(c_summary)\n",
    "  print()\n",
    "  print(report)\n",
    "  print()\n",
    "  print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 527,
     "output_extras": [
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3374,
     "status": "ok",
     "timestamp": 1520160169744,
     "user": {
      "displayName": "Antonio Feregrino",
      "photoUrl": "//lh6.googleusercontent.com/-Z1ZALTIQdzQ/AAAAAAAAAAI/AAAAAAAACzA/_Q6v8PNnt7Q/s50-c-k-no/photo.jpg",
      "userId": "108375679337900838293"
     },
     "user_tz": 0
    },
    "id": "bwDZ0LV7VCsz",
    "outputId": "25b28d65-cf2c-4e9d-bae3-6059826619c5"
   },
   "outputs": [],
   "source": [
    "summary, report, matrix = eval_summary(predictions, test_posts['discourse_type'])\n",
    "\n",
    "c_report = dataframe_from_report(report, 'subreddit')\n",
    "\n",
    "datos = \"old_subredd_function\"\n",
    "\n",
    "c_report.to_csv('report_q4_%s.csv' % datos)\n",
    "files.download ('report_q4_%s.csv' % datos)\n",
    "\n",
    "c_matrix = pd.DataFrame(matrix, columns=c_report.index, index=c_report.index)\n",
    "print('confusion_matrix_q4_%s.csv' % datos)\n",
    "c_matrix.to_csv('confusion_matrix_q4_%s.csv' % datos)\n",
    "files.download ('confusion_matrix_q4_%s.csv' % datos)\n",
    "\n",
    "c_summary = pd.DataFrame.from_dict({\n",
    "    'accuracy' : [summary[0]],\n",
    "    'precission': [summary[1]],\n",
    "    'recall': [summary[2]],\n",
    "    'f1': [summary[3]]\n",
    "})\n",
    "\n",
    "c_summary.to_csv('summary_q4_%s.csv' % datos)\n",
    "files.download ('summary_q4_%s.csv' % datos)\n",
    "\n",
    "print(c_summary)\n",
    "print()\n",
    "print(report)\n",
    "print()\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "svqbMhV-lcEE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Coursework.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
